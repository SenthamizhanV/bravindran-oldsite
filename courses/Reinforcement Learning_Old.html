<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">



<title>Balaraman Ravindran</title>
<meta name="keywords" content="">
<meta name="description" content="">
<link href="./Reinforcement Learning_files/default.css" rel="stylesheet" type="text/css">
</head><body>
<div id="header">
<center>
	<div id="logo">
		<!--<h1>Balaraman Ravindran</h1>-->
	<a href="http://www.cse.iitm.ac.in/~ravi/Ravi-Pic.jpg"><img src="./Reinforcement Learning_files/Ravi-Pic-small.jpg" vspace="25" height="150" align="middle"></a>
	</div>
</center>
	<div id="address">
<font size="+2">Balaraman Ravindran</font><br>
<!--BSB 349--><br>
Computer Science and Engineering &<br>
Robert Bosch Centre for Data Science and AI<br>
Indian Institute of Technology Madras<br>
Chennai, 600 036.<br>
Ph: +91-44-22574370<br>
ravi@cse.iitm.ac.in<br>
		<ul>
			<li><a href="http://www.cse.iitm.ac.in/~ravi/index.html" accesskey="1" title="">Home</a></li>
			<li><a href="http://www.cse.iitm.ac.in/~ravi/publications.html" accesskey="2" title="">Publications</a></li>
			<li><a href="http://www.cse.iitm.ac.in/~ravi/research.html" accesskey="3" title="">Research</a></li>
			<li><a href="http://www.cse.iitm.ac.in/~ravi/teaching.html" accesskey="4" title="">Teaching</a></li>
			<li><a href="http://www.cse.iitm.ac.in/~ravi/students.html" accesskey="5" title="">Students</a></li>
		</ul>
	</div>
</div>
<div id="content">
	<div id="colOne">
		<div class="content">
<h4><a href="./index.html">Online NPTEL Courses</a></h4>		<!-- replace with web link -->
(<font size=1><a href="../teaching.html">Classroom Courses Taught</a></font>)</b><br>
<ul>
<h4>Reinforcment Learning</h4><p></p>
Reinforcement learning is a paradigm that aims to model the 
trial-and-error learning process that is needed in many problem 
situations where explicit instructive signals are not available. It has 
roots in operations research, behavioral psychology and AI. The goal of 
the course is to introduce the basic mathematical foundations of 
reinforcement learning, as well as highlight some of the recent 
directions of research.
<br>
<font size="1">Previously offered: <a href="http://nptel.ac.in/courses/106106143/">July-October 2016</a>.</font>

<font size="2"><ul>
	<li><p><b>Week 0 - Preparatory Material</b></p></li>
		<ul>
			<li>&nbsp;&nbsp; <a href="https://youtu.be/4CgVEt-BhLA">Probability tutorial - 1</a></li>
			<li>&nbsp;&nbsp; <a href="https://youtu.be/NFQ94cd649M">Probability tutorial - 2</a></li>
			<li>&nbsp;&nbsp; <a href="https://youtu.be/lWSYxvJbf9E">Linear algebra tutorial - 1</a></li>
			<li>&nbsp;&nbsp; <a href="https://youtu.be/wW4tIx9dOk8">Linear algebra tutorial - 2</a></li>
			<li>&nbsp;&nbsp; <a href="./Reinforcement Learning_files/Assignment0.pdf">Assignment 0</a></li>
			<li>&nbsp;&nbsp; <a href="./Reinforcement Learning_files/Solution0.pdf">Solution 0</a></li>
		</ul>
	<li><p><b>Week 1 - Introduction to RL and Immediate RL</b></p></li>
		<ul>
			<li>&nbsp;&nbsp; <a href="https://youtu.be/YaPSPu7K9S0">Introduction to RL</a></li>
			<li>&nbsp;&nbsp; <a href="https://youtu.be/ewkm38skUlY">RL framework and applications</a></li>
			<li>&nbsp;&nbsp; <a href="https://youtu.be/nNK4xKkAapM">Introduction to immediate RL</a></li>
			<li>&nbsp;&nbsp; <a href="https://youtu.be/AizR8uvhX-s">Bandit optimalities</a></li>
			<li>&nbsp;&nbsp; <a href="https://youtu.be/iB4m-jP7blI">Value function based methods</a></li>
			<li>&nbsp;&nbsp; <a href="./Reinforcement Learning_files/Assignment1.pdf">Assignment 1</a></li>
			<li>&nbsp;&nbsp; <a href="./Reinforcement Learning_files/Solution1.pdf">Solution 1</a></li>
		</ul>
	<li><p><b>Week 2 - Bandit Algorithms</b></p></li>
		<ul>
			<li>&nbsp;&nbsp; <a href="https://youtu.be/TIlDzLZPyhY">UCB 1</a></li>
			<li>&nbsp;&nbsp; <a href="https://youtu.be/KB-ZDvLbuOQ">Concentration bounds</a></li>
			<li>&nbsp;&nbsp; <a href="https://youtu.be/7IA6Zb7KZM0">UCB 1 Theorem</a></li>
			<li>&nbsp;&nbsp; <a href="https://youtu.be/N_9HgdhXKIY">PAC bounds</a></li>
			<li>&nbsp;&nbsp; <a href="https://youtu.be/3iNaR0Mq1ug">Median elimination</a></li>
			<li>&nbsp;&nbsp; <a href="https://youtu.be/H2OWTxdauqA">Thompson sampling</a></li>
			<li>&nbsp;&nbsp; <a href="./Reinforcement Learning_files/Assignment2.pdf">Assignment 2</a></li>
			<li>&nbsp;&nbsp; <a href="./Reinforcement Learning_files/Solution2.pdf">Solution 2</a></li>
		</ul>
	<li><p><b>Week 3 - Policy Gradient Methods & Introduction to Full RL</b></p></li>
		<ul>
			<li>&nbsp;&nbsp; <a href="https://youtu.be/y3QEOmkxsQo">Policy search</a></li>
			<li>&nbsp;&nbsp; <a href="https://youtu.be/WIBWQ7lOXoA">REINFORCE</a></li>
			<li>&nbsp;&nbsp; <a href="https://youtu.be/K2Hh-ayvsJU">Contextual bandits</a></li>
			<li>&nbsp;&nbsp; <a href="https://youtu.be/yIdgKWFulNQ">Full RL introduction</a></li>
			<li>&nbsp;&nbsp; <a href="https://youtu.be/BiY_X1c068U">Returns, value functions & MDPs</a></li>
			<li>&nbsp;&nbsp; <a href="./Reinforcement Learning_files/Assignment3.pdf">Assignment 3</a></li>
			<li>&nbsp;&nbsp; <a href="./Reinforcement Learning_files/Solution3.pdf">Solution 3</a></li>
		</ul>
	<li><p><b>Week 4 - MDP Formulation, Bellman Equations & Optimality Proofs</b></p></li>
		<ul>
			<li>&nbsp;&nbsp; <a href="https://youtu.be/BpSNh1h4HeQ">MDP modelling</a></li>
			<li>&nbsp;&nbsp; <a href="https://youtu.be/CTPHADvQxSs">Bellman equation</a></li>
			<li>&nbsp;&nbsp; <a href="https://youtu.be/4S9aRrsP954">Bellman optimality equation</a></li>
			<li>&nbsp;&nbsp; <a href="https://youtu.be/THmztE-ghWc">Cauchy sequence & Green's equation</a></li>
			<li>&nbsp;&nbsp; <a href="https://youtu.be/EKwc7rw9YCU">Banach fixed point theorem</a></li>
			<li>&nbsp;&nbsp; <a href="https://youtu.be/9UhK8U6rEVY">Convergence proof</a></li>
			<li>&nbsp;&nbsp; <a href="./Reinforcement Learning_files/Assignment4.pdf">Assignment 4</a></li>
			<li>&nbsp;&nbsp; <a href="./Reinforcement Learning_files/Solution4.pdf">Solution 4</a></li>
		</ul>
	<li><p><b>Week 5 - Dynamic Programming & Monte Carlo Methods</b></p></li>
		<ul>
			<li>&nbsp;&nbsp; <a href="https://youtu.be/ybEyXc4hNuk">LPI convergence</a></li>
			<li>&nbsp;&nbsp; <a href="https://youtu.be/F3DjpixO1bY">Value iteration</a></li>
			<li>&nbsp;&nbsp; <a href="https://youtu.be/09L0nqw9Vnc">Policy iteration</a></li>
			<li>&nbsp;&nbsp; <a href="https://youtu.be/_0BQZyXx0eU">Dynamic programming</a></li>
			<li>&nbsp;&nbsp; <a href="https://youtu.be/DaxOiIdjQ4Y">Monte Carlo</a></li>
			<li>&nbsp;&nbsp; <a href="https://youtu.be/h7lfqnV-8Pc">Control in Monte Carlo</a></li>
			<li>&nbsp;&nbsp; <a href="./Reinforcement Learning_files/Assignment5.pdf">Assignment 5</a></li>
			<li>&nbsp;&nbsp; <a href="./Reinforcement Learning_files/Solution5.pdf">Solution 5</a></li>
		</ul>
	<li><p><b>Week 6 - Monte Carlo & Temporal Difference Methods</b></p></li>
		<ul>
			<li>&nbsp;&nbsp; <a href="https://youtu.be/oHE7u15n7Qg">Off Policy MC</a></li>
			<li>&nbsp;&nbsp; <a href="https://youtu.be/unCdZCQS64g">UCT</a></li>
			<li>&nbsp;&nbsp; <a href="https://youtu.be/4KlVxH8wMWQ">TD(0)</a></li>
			<li>&nbsp;&nbsp; <a href="https://youtu.be/x3PgPnVC-bQ">TD(0) control</a></li>
			<li>&nbsp;&nbsp; <a href="https://youtu.be/ioQz9Pkycb0">Q-learning</a></li>
			<li>&nbsp;&nbsp; <a href="https://youtu.be/w3wGvwi336I">Afterstate</a></li>
			<li>&nbsp;&nbsp; <a href="./Reinforcement Learning_files/Assignment6.pdf">Assignment 6</a></li>
			<li>&nbsp;&nbsp; <a href="./Reinforcement Learning_files/Solution6.pdf">Solution 6</a></li>
		</ul>
	<li><p><b>Week 7 - Eligibility Traces</b></p></li>
		<ul>
			<li>&nbsp;&nbsp; <a href="https://youtu.be/n0K83V1sCxc">Eligibility traces</a></li>
			<li>&nbsp;&nbsp; <a href="https://youtu.be/A1s3dc64uhE">Backward view of eligibility traces</a></li>
			<li>&nbsp;&nbsp; <a href="https://youtu.be/OgKysheQJSQ">Eligibility trace control</a></li>
			<li>&nbsp;&nbsp; <a href="https://youtu.be/uGUAUkmqQtk">Thompson sampling recap</a></li>
			<li>&nbsp;&nbsp; <a href="./Reinforcement Learning_files/Assignment7.pdf">Assignment 7</a></li>
			<li>&nbsp;&nbsp; <a href="./Reinforcement Learning_files/Solution7.pdf">Solution 7</a></li>
		</ul>
	<li><p><b>Week 8 - Function Approximation</b></p></li>
		<ul>
			<li>&nbsp;&nbsp; <a href="https://youtu.be/gqSuPgrcVx8">Function approximation</a></li>
			<li>&nbsp;&nbsp; <a href="https://youtu.be/c4cvheE3diA">Linear parameterization</a></li>
			<li>&nbsp;&nbsp; <a href="https://youtu.be/wGPxro6C2ik">State aggregation methods</a></li>
			<li>&nbsp;&nbsp; <a href="https://youtu.be/N21zCqTwxbs">Function approximation & eligibility traces</a></li>
			<li>&nbsp;&nbsp; <a href="https://youtu.be/zTFNYrYbvVE">LSTD & LSTDQ</a></li>
			<li>&nbsp;&nbsp; <a href="https://youtu.be/JmbN6XZ4Cro">LSPI & Fitted Q</a></li>
			<li>&nbsp;&nbsp; <a href="./Reinforcement Learning_files/Assignment8.pdf">Assignment 8</a></li>
			<li>&nbsp;&nbsp; <a href="./Reinforcement Learning_files/Solution8.pdf">Solution 8</a></li>
		</ul>
	<li><p><b>Week 9 - DQN, Fitted Q & Policy Gradient Approaches</b></p></li>
		<ul>
			<li>&nbsp;&nbsp; <a href="https://youtu.be/AohtWrJwvgg">DQN & Fitted Q-iteration</a></li>
			<li>&nbsp;&nbsp; <a href="https://youtu.be/f2l8bv8GH5U">Policy gradient approach</a></li>
			<li>&nbsp;&nbsp; <a href="https://youtu.be/sTZ4GyJ4FZU">Actor critic & REINFORCE</a></li>
			<li>&nbsp;&nbsp; <a href="https://youtu.be/BEhnse8cLDw">REINFORCE (cont'd)</a></li>
			<li>&nbsp;&nbsp; <a href="https://youtu.be/X6yCRaQa5FE">Policy gradient with function approximation</a></li>
			<li>&nbsp;&nbsp; <a href="./Reinforcement Learning_files/Assignment9.pdf">Assignment 9</a></li>
			<li>&nbsp;&nbsp; <a href="./Reinforcement Learning_files/Solution9.pdf">Solution 9</a></li>
		</ul>
	<li><p><b>Week 10 - Hierarchical Reinforcement Learning</b></p></li>
		<ul>
			<li>&nbsp;&nbsp; <a href="https://youtu.be/K5MlmO0UJtI">Hierarchical reinforcement learning</a></li>
			<li>&nbsp;&nbsp; <a href="https://youtu.be/EbTIQqVDJsg">Types of optimality</a></li>
			<li>&nbsp;&nbsp; <a href="https://youtu.be/vdRG-EbcNGw">Semi-Markov decision processes</a></li>
			<li>&nbsp;&nbsp; <a href="https://youtu.be/1eb4vECGKC4">Options</a></li>
			<li>&nbsp;&nbsp; <a href="https://youtu.be/6wH3yS07NTA">Learning with options</a></li>
			<li>&nbsp;&nbsp; <a href="https://youtu.be/rVhb6f6G-1M">Hierarchical abstract machines</a></li>
			<li>&nbsp;&nbsp; <a href="./Reinforcement Learning_files/Assignment10.pdf">Assignment 10</a></li>
			<li>&nbsp;&nbsp; <a href="./Reinforcement Learning_files/Solution10.pdf">Solution 10</a></li>
		</ul>
	<li><p><b>Week 11 - Hierarchical RL: MAXQ</b></p></li>
		<ul>
			<li>&nbsp;&nbsp; <a href="https://youtu.be/4mmW0kYmK3I">MAXQ</a></li>
			<li>&nbsp;&nbsp; <a href="https://youtu.be/1iACStiJQBs">MAXQ value function decomposition</a></li>
			<li>&nbsp;&nbsp; <a href="https://youtu.be/uJ0oNtDG6ro">Option discovery</a></li>
			<li>&nbsp;&nbsp; <a href="./Reinforcement Learning_files/Assignment11.pdf">Assignment 11</a></li>
			<li>&nbsp;&nbsp; <a href="./Reinforcement Learning_files/Solution11.pdf">Solution 11</a></li>
		</ul>
	<li><p><b>Week 12 - POMDPs</b></p></li>
		<ul>
			<li>&nbsp;&nbsp; <a href="https://youtu.be/9G_KevA8DFY">POMDP introduction</a></li>
			<li>&nbsp;&nbsp; <a href="https://youtu.be/dMOUp7YzUpQ">Solving POMDP</a></li>
			<li>&nbsp;&nbsp; <a href="./Reinforcement Learning_files/Assignment12.pdf">Assignment 12</a></li>
			<li>&nbsp;&nbsp; <a href="./Reinforcement Learning_files/Solution12.pdf">Solution 12</a></li>
		</ul>
</ul></font>

</ul>
</div> </div> <div id="colTwo"> 
               <h4 class="top-head">Currently Teaching</h4>
                <div class="content">
                <p>
                </p><ul>                
		<font size="2"><li> <a href="https://onlinecourses.nptel.ac.in/noc17_mg24/preview">Introduction to Data Analytics</a></li></font>

                <div class="content">
</div>
			<div class="content"><form method="get" action="http://www.google.com/search"> <input name="q" size="17" maxlength="255" type="text"> <input name="btnG" value="Google Search" type="submit"> </form></div>
<font size="1">	</font></ul></div><font size="2">
</font></div><font size="2">
<div id="footer" style="clear: both;">
<font size="1">	</font><p><font size="1">Copyright 2009 Balaraman Ravindran. Designed based on a template by  <a href="http://freecsstemplates.org/"><strong>Free CSS Templates</strong></a> and <a href="http://people.csail.mit.edu/kersting/index.html">Kristian Kersting</a></font></p>
</div>

</font>
</div>
</body></html>
