<html>
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
</head>
<body alink="#56bb55" bgcolor=wheat link="#0000cc" text="#000000" vlink="#000077">
<dl>
<dd>
</dd>
<dd><a name="saravanan"></a></dd>
<dt>
<b>
<h2>PhD</h2>
</b>
</dt>
<dd><b> M. Saravanan (2008)</b></dd>
<dd>
<b>Thesis title: </b>Ontology-Based Retrieval and Automatic Summarization of Legal Judgments 
</dd>
<dd> Ericsson R &D, Chennai </dd>
<br>
<dd> <i>ABSTRACT</i> </dd>
<br>
<dd>
In the current scenario of information overload, the most challenging task before the computing society is to devise methods for efficient retrieval and
timely delivery of relevant information in short and precise segments to the end user. The increasing availability of legal judgments in digital form
creates opportunities and challenges for both the legal community and for information technology researchers. While digitized documents facilitate easy
access to a large number of documents, finding all documents that are relevant to the task at hand and comprehending a vast number of them are non-trivial
tasks. In this thesis we address the issues of legal judgment retrieval and of aiding in rapid comprehension of the retrieved documents. The usual practice of the legal community is that of reading the summaries (headnotes) instead of reading the entire judgments. Moreover, the legal community is also in search
of an end-to-end legal information retrieval system which can give a solution for their day- to-day activities. In our thesis, a system has been proposed and tested for creating headnotes automatically from the retrieved legal judgments.
    Rhetorical roles are used to represent the collection of sentences under common titles, according to their function in a narrative. Imposing a structure on a document using such roles greatly aids in comprehension. In our study, seven rhetorical roles namely identifying the case, establishing the facts of the case, arguing the case, history of the case, arguments, ratio decidendi and final decision have been identified for this purpose. We pose the problem of performing genre analysis for identifying these roles present in a judgment as one of automatic segmentation of a document. Conditional Random Fields, a discriminative graphical model has been employed in our work for text segmentation and the resulting labels are used to structure the judgments and the corresponding headnotes to aid in better comprehension.
       The other major problem we address is that of retrieval of judgments relevant to the cases a legal user is currently involved in. To facilitate this we have developed a novel framework for constructing a legal knowledge base in the form of a legal ontology. Ontologies ensure efficient retrieval of resources by enabling inferences based on domain knowledge. The knowledge base developed is used to enhance the query given by the user in order to retrieve more relevant judgments. The retrieved judgments are considered for generating a final summary.
        We have used term distribution model to extract important sentences from the legal judgments for the automatic summarization task. Another major issue to be handled in our study is to generate a “user-friendly” summary at the end. The rhetorical roles identified in the earlier phase have been used to improve the final summary. That is, we can significantly improve extraction-based summarization to generate coherent and concise outputs that incorporate important features of the legal judgment, namely the ratio decidendi and final decision.
        The documents considered for study in this thesis are from three different sub-domains viz. rent control, income tax and sales tax related to civil court judgments. The proposed model was evaluated on a specific data collection spanning three legal sub domains. The performances of our system and other auto-summarizers available in the public domain were compared with the summary generated by a set of human subjects. It is found that, at different stages, our system-generated output is close to the outputs generated by human subjects, and it is better than the other tools considered in the study. Thus, the present work comprises almost all the aspects of finding relevant information in the document space for helping the legal communities in their information needs.
</dd>
</dd>
<br>
<br>


<dd><a name="sriram"></a></dd>
<br>
<br>
<dt> <b><h2>MS</h2></b>
</dt><dd> <b>Sriram Raghavan (2007)</b>
<br>
<dd> <b>Thesis title:</b> Distributed Algorithms for Hierarchical Area Coverage Using Teams of Homogeneous Robots</dd>
<dd>PhD, QUT</dd>
<br>
<dd><i>ABSTRACT</i></dd>
<br>
<dd>
Covering a given area using a team of mobile robots poses several challenges. One such challenge lies in guaranteeing efficient coverage in
the absence of complete information. The robots (agents) must cover the area in a manner that the overlap (repeated coverage) in coverage across
all the robots is minimized.
	In this thesis, we introduce “overlap_ratio” to explicitly measure the overlap in coverage for various algorithms. This measure has been shown
to adequately represent the performance of any coverage system as it effectively captures the resource usage as well as the time required for
coverage. We show systematically how the area coverage algorithms, which perform complete coverage with minimum overlap, can be designed
for a team of mobile robots. We begin by understanding the behavior of robots in a random-walk model and successively refine the model to
provide the robots with additional capabilities and minimize the overlap.
	We gradually transition from random decision-making to deterministic decision-making and suggest appropriate algorithms to suit various
application needs and capabilities. We also prove that arbitrarily large areas can be covered with simple and elegant coverage algorithms by
hierarchically composing it using smaller areas called primitives. An associated theorem called the H2C theorem that provides a linear scaling
of overlap ratio with exponential increase in area has been proved. 
	Further, this theorem is applicable across arbitrary number of levels in hierarchy. We demonstrate the same experimentally through simulation.
Performance of such multi-robot (agent) applications critically depends on the communication architecture that facilitates coordination. A generic
architecture often turns out to be burdensome on the robot (agent) due to overhead. With significant increase in the number of applications, a lightweight architecture that supports reliable and robust delivery of coordination messages is the need of the hour. We explain the design of a
Multi-Robot Coordination Architecture, called Pseudonet, based on Bluetooth, to develop area coverage algorithms through successive
refinement. Pseudonet supports a wide variety of multi-agent applications in which communication is the primary mode of coordination. We
illustrate the use of Pseudonet architecture in the multi-robot area coverage application.
</dd>
<br>
<br>
<br>
<br>
<dd><a name="sreenivas"></a></dd>
<br>
<br>
<dt><dd><b> B. H. Sreenivasa Sarma (2007)
</b>
</dd> </dt>
<dd><b>Thesis title: </b>Intelligent Tutoring System using Reinforcement Learning 
</dd>
<dd>LSI Technologies, Bangalore
</dd>
<br>
<dd>
<i>
ABSTRACT
</i>
<br><br>
Intelligent Tutoring System (ITS) is an interactive and effective way of teaching students. ITS gives instructions about a topic to a student, who is using it. The student has to learn the topic from an ITS by solving problems. The system gives a problem and compares the solution it has with that of the student’s and then it evaluates the student based on the differences. The system continuously updates a student model by interacting with the student. Usually, ITS uses artificial intelligence techniques to customize its instructions according to the student’s need. For this purpose the system should have the knowledge of the student (student model) and the set of pedagogical rules. Disadvantages of these methods are, it is hard to encode all pedagogical rules, it is difficult to incorporate the knowledge that human teachers use and it is difficult to maintain a student model and also these systems are not adaptive to new student’s behavior. To overcome these disadvantages we use Reinforcement Learning (RL) to take teaching actions. RL learns to teach, given only the state description of the student and reward. Here, the student model is just the state description of the student, which reduces the cost and time of designing ITS. The RL agent uses the state description as the input to a function approximator, and uses the output of the function approximator and reward, to take a teaching action. To validate and to improve the ITS we have experimented with teaching a simulated student using the ITS. We have considered issues in teaching a special case of students suffering from autism. Autism is a disorder of communication, socialization and imagination. An Artificial Neural Network (ANN) with backpropagation is selected as simulated student, that shares formal qualitative similarities with the selective attention and generalization deficits seen in people with autism. Our experiments show that an autistic student can be taught as effectively as a normal student using the proposed ITS.
</dd>
<br>
<br>
<dd><a name="vimal"></a></dd>
<br>
<br>
<dt><dd> <b>Vimal Mathew (2007)</b>
</dd> </dt>
<dd><b>Thesis title:</b> Automated Spatio-Temporal Abstraction in Reinforcement Learning </dd>
<dd>PhD, UMass Amherst</dd>
<br><dd>
<i>
ABSTRACT
</i>
<br>
<br>
Reinforcement learning is a machine learning framework in which an agent manipulates its environment through a series of actions, receiving a scalar feedback signal or reward after each action. The agent learns to select actions so as to maximize the expected total reward. The nature of the rewards and their distribution across the environment decides the task that the agent learns to perform. Consider a generic agent solving multiple tasks in a common environment. The effectiveness of such an agent increases with its ability to reuse past experience in new tasks. Identifying similar representations
between tasks paves the way for a possible transfer of knowledge between such tasks. A sufficiently broad definition of similarity and an ability to effectively store and reuse previous experience can greatly increase the long-term performance of a multipurpose agent.
    This thesis introduces an automated task-independent abstraction method that generalizes across tasks sharing the same state-space topology and similar dynamics. New tasks performed on this common environment are able to reuse skills acquired over previous tasks. Pruning techniques are identified for specific situations to identify relevant skills. The interaction of the agent with the environment is modeled as a dynamical system. Identification of metastable regions in the agent dynamics leads to a partitioning of the state space. Transitions between metastable regions, which normally are events of low probability during a random-walk on the state space, are identified as useful skills.
</dd>
<br>
<br>
<dd><a name="shravan"></a></dd>
<br>
<br>
<dt><dd><b>Shravan Matthur Narayanamurthy (2007)</b>
</dd> </dt>
<dd><b>Thesis title:</b> Abstraction using Symmetries in Markov Decision Processes</dd>
<dd>Yahoo Labs, Bangalore</dd>
<dd>
<br>
<i>ABSTRACT</i>
<br><br>
Current approaches to solving Markov Decision Processes (MDPs), the de-facto standard for modeling stochastic sequential decision problems, scale poorly with the size of the MDP. When used to model real-world problems though, MDPs exhibit considerable implicit redundancy, especially in the form of symmetries. However, existing model minimization approaches do not efficiently exploit this redundancy due to symmetries. These approaches involve constructing a reduced model first and then solving them. Hence we term these as “explicit minimization” approaches. In the first part of this work, we address the question of finding symmetries of a given MDP. We show that the problem is Isomorphism Complete, that is, the problem is polynomially equivalent to verifying whether two graphs are isomorphic. Apart from the theoretical importance of this result it has an important practical application. The reduction presented can be used together with any off-the-shelf Graph Isomorphism solver, which performs well in the average case, to find symmetries of an MDP. In fact, we present results of using NAutY (the best Graph Isomorphism solver currently available), to find symmetries of MDPs. We next address the issue of exploiting the symmetries of a given MDP. We propose the use of an explicit model minimization algorithm called the G-reduced image algorithm that exploits symmetries in a time efficient manner. We present an analysis of the algorithm and corroborate it with empirical results on a probabilistic GridWorld domain and a single player GridWorld Soccer domain. We also present results of integrating the symmetry finding with the explicit model minimization approach to illustrate an end-to-end approach for “Abstraction using Symmetries in MDPs”. We then note some of the problems associated with this explicit scheme and as a solution present an approach wherein we integrate the symmetries into the solution technique implicitly. However, we should select a suitable solution technique so that the overheads due to integration do not outweigh the improvements. We validate this approach by modifying the Real Time Dynamic Programming (RTDP) algorithm and empirically demonstrate significantly faster learning and reduced
overall execution time on several domains.
</dd>
<br>
<br>
<dd><a name="aakanksha"></a></dd>
<br>
<br>
<dt><dd><b>Aakanksha Gagrani (2007) (Jointly with Prof. Koshy Varghese, CE, IITM)</b>
</dd> </dt>
<dd><b>Thesis title: </b>Image Modeling using Hierarchical Conditional Random Fields </dd>
<dd>Oracle, Hyderabad</dd>
<br>
<dd>
<i>ABSTRACT</i>
<br><br>
Image modeling methods broadly fall under two categories, (a) Methods which consider bottom up cues , these methods take pixel statistics into consideration
for classification. (b) Methods which consider top down cues as well, these methods utilize the context information as well for the modeling task. Natural images exhibit strong contextual dependencies in the form of spatial interactions among components. For example, neighboring pixels tend to have similar class labels, and different parts of an object are related through geometric constraints. Going beyond these, different regions e.g., objects such as, monitor and keyboard appear in restricted spatial configurations. Modeling these interactions is crucial for achieving good classification accuracy.
    In this thesis, we present a method based on Hierarchical Conditional Random Fields that can handle both bottom up and top down cues simultaneously. This offers tremendous advantages over existing methods which use either bottom up cues, or model only limited contextual information. The Tree Structured Conditional Random Field (TCRF) as proposed, is capable of capturing long range correlations at various levels of scales. TCRF has non loopy graph structure whichallows us to perform inference in time linear in number of nodes. The model is generic enough to be applied to several challenging computer vision tasks, such as object detection and image labeling; seamlessly within a single, unified framework.
</dd>
<br>
<br>
<dd><a name="dinakar"></a></dd>
<br>
<br>
<dt><dd><b>Dinakar Jayarajan (2009)(Jointly with Dr. Dipti Deodhare, CAIR-DRDO)</b>
</dd> </dt>
<dd><b>Thesis title: </b>Using Semantics in Document Representation: A Lexical Chains Approach</dd>
<dd>PhD, USC</dd>
<br>
<dd>
<i>ABSTRACT</i>
<br><br>
Automatic classification and clustering are two of the most common operations performed on text documents. Numerous algorithms have been proposed for this and invariably, all of these algorithms use some variation of the vector space model to represent the documents. Traditionally, the Bag of Words (BoW) representation is used to model the documents in a vector space. The BoW scheme is a simple and popular scheme, but it suffers from numerous drawbacks. The chief among them is that the feature vectors generated using BoW results in very large dimensional vectors. This creates problems with most machine learning algorithms where the high dimensionality severely affects the performance. This fact is manifested in the current thinking in the machine learning community, that some sort of a dimensionality reduction is a beneficial preprocessing step for applying machine learning algorithms on textual data. BoW also fails to capture the semantics contained in the documents properly.
    In this thesis, we present a new representation for documents based on lexical chains. This representation addresses both the problems with BoW - it achieves a significant reduction in the dimensionality and captures some of the semantics present in the data. We present an improved
algorithm to compute lexical chains and generate feature vectors using these chains. We evaluate our approach using datasets derived from the 20 Newsgroups corpus. This corpus is a collection of 18,941 documents across 20 Usenet groups related to topics such as computers, sports, vehicles, religion, etc. We compare the performance of the lexical chain based document features against the BoW features on two machine learning tasks - clustering and classification. We also present a preliminary algorithm for soft clustering using the lexical chains based representation, to facilitate topic detection from a document corpus.
</dd>
<br>
<br>
<dd><a name="ankit"></a></dd>
<dt><b><h2>M.Tech</h2></b>
<br>
<br>
<dt><dd><b>Ankit Malpani (Dual Degree) (Jointly with Prof. Hema A. Murthy)</b>
</dd> </dt>
<dd><b>Thesis title:</b>Personalized Intelligent Tutoring System </dd>
<dd>MSIDC, Hyderabad</dd>
<br>
<dd>
<i>ABSTRACT</i>
<br><br>
Throughout the educational system, teaching has traditionally followed a one-size-fits-all approach to learning, with a single set of instructions provided identically to everybody in a given class,regardless of differences in aptitude or interest.
	In recent years,a growing appreciation of individual preferences and aptitudes has led towards Personalized Intelligent Tutoring System where instructions are presented to students based on theisr needs, adapting to their learning behaviour to make learning both interesting and challenging. The US National Academy of Engineers identified it as one of the fourteen grand challenges for engineering in the 21st century. The system becomes even more relevant in the current Indian Educational scenario. In rural arears skilled and inspiring teaches are few and far to come by. An Intelligent Tutoring system can go a long way in bringing in good students from the marginalized sections of society into the main stream.
	Most of the present tutoring systems use extensive expert knowledge to either explicitly store teaching rules or have a finely labelled dataset specifying skills extending such a Tutoring System to newer domains difficult specially in India with numerous education boards and poorly labelled data-sets.
	In this project, we propose a system that works on coarsely labelled data-sets and uses Reinforcement Learning techniques to implicitly learn teaching rules. We use Student Models to simulate student's learning behaviour and show that our system helps in speeding up students learning process. As interactions with the student proceed the system also learns to identify between different types of students and presents instructions to suit individual student's need.
</dd>
<br>
<br>
<dd><a name="kiran"></a></dd>
<br>
<br>
<dt><dd><b>Kiran Kate </b>
</dd> </dt>
<dd><b>Thesis title: </b>Positional Analysis of Social Networks</bb></dd>
<dd> IBM-IRL, Bangalore</dd>
<br>
<dd>
<i>ABSTRACT</i>
<br>
<br>
Social network analysis is an emerging area of research for computer scientists. It employs different concepts from graph theory, probability, and statistics to solve problems in a wide range of disciplines. Social network analysis can be performed on many real world networks from different domains.There are a number of social network analysis methods designed for various tasks and positional analysis is one of them.
	It involves partitioning of the set of actors into subsets such that actors in a subset are similar in their structural relationships with other actors. Traditional methods of positional analysis are either too strict or too loose. Their applicability to real world large graphs is severely limited as they result into almost trival partitions. We propose a useful relaxation to the concept of equitable partition called as e-equitable partition. A variant of epsilon equitable partition called maximal e-equitable partition is also proposed and formulated as an optimization problem. Various heuristics to compute a maximal e-equitable partition of a graph were tried and finally,a fast algorith is proposed.
	We also present the results of performing positional analysis on a number of networks and demonstrate empirically that positional analysis with our notion gives raise to non-trivial and meaningful partitions. Along with the static network anaysis with respect to positions, we have also studied the impact of positional analysis on the evolution of networks. Our results show that positional analysis indeed plays an important role in the evolution.
</dd>
<br>
<br>
<dd><a name="rachit"></a></dd>
<br>
<br>
<dt><dd><b>Rachit Arora </b>
</dd> </dt>
<dd><b>Thesis title: </b>Latent Dirichlet Allocation and Singular Value Decomposition based Multi-Document Summarization</dd>
<dd> Microsoft, Redmond </dd>
<br><dd>
<i>ABSTRACT</i>
<br><br>
Mixture models like Latent Dirichlet Allocation are used to represent documents as a mathematical model with a given probability distribution. We look at a novel way of applying this to one of the most challenging task in Statistical Natural Language Processing multi-document summarization. Representing the documents as a probability distribution using LDA can effectively capture and represent the topics of the corpus. We also look at the methods of improving the pre-existing algorithm for estimation and interference in LDA and give certain work-arounds for the limitations in the LDA Model which can be used in other topic models which are extension of the basic LDA model.
	Singular Value Decomposition is an effective tool of Principal Component Analysis used in the area of Text Retrieval. It is used to find orthogonal dimensions for the terms and we exploit this ability to find orthogonal dimensions to extract sentences orthogonal to each other and form a summary. In PCA one of the problems being looked into is a weighting scheme for the terms of the term-document matrix. The current weighting mechanisms dont capture the underlying structure that the documents represent as a set. they are based on the assumption that the documents are independant of each other. We show that improvements can be made by making the assumption that is central to the LDA theme that documents share a certain set of underlying latent topics. Using the probality distribution of the LDA model we see a way of weighting the terms of the documents and show its application by computing multi-document summary using this combined LDA-SVD model.
	LDA based topic models are very effective in capturing the correlations among the terms at the document level. Howvere many tasks in SNLP deal with the documents at the level of the sentences. The terms have stronger correlation among them at the sentence level and lesser at the document level. Thus we have come up with a new topic model, which we call Latent Dirichlet Segment and word allocation that captures the correlation among the terms of the documents both at the level of sentences or paragraphs and at the level of the documents. Thus we represent the documents of a corpus by capturing there underlying structure of latent topics, which is the theme of LDA and capturing the structure of the documents which is represented by the sentences of the documents.
	Finally we look at a News Aggregator in which we attempt to gather the common news articles from various websites and present them to the user under one heading. Thus a user can read the same news event of his favorite new websites and also compare them at one place. Together with news articles we can provide the user even more facilities like timeline and summary for the entire news event or the individual articles.
</dd>
<br>
<br>
<dd><a name="simar"></a></dd>
<br>
<br>
<dt><dd><b>Simarjit Singh  </b>
</dd> </dt>
<dd><b>Thesis title:</b>Policy Learning in Partially Observable Environment </dd>
<dd>PGP, IIM Lucknow  </dd>
<br><dd>
<i>ABSTRACT</i>
<br><br>
Agents that act in real environments,whether physical or virtual, rarely have complete information about the state of environment in which they are working. Robots and complex systems have limitations in sensor capabilities as well as bottle necks in computational power,these limitations further restrict the real time information available to agent for decision-making. Predictive State Representation (PSRs) is a recently introduced class of models for discrete time dynamical systems with partial observability of the actual state of system. PSRs replace the traditional notion of state with a set of statistics about future actions and observations. PSRs have been extended to encompass continuous domains,since a lot of real world signals are continous. But the problem is the quality of the predicitions are dependant on the core test cases chosen, and in continous domains,there is not a good criterion yat, like linear independant in discrete domains. We have extended the existing literature in PSR to address the problems of number of test cases and test case length. We have also provided a firm theoretical foundation using Schwarz information theoretic criteria for test case selection. By modeling problems from field of economics in RL domain, we have shown our criterion for selection of test case length and count indeed provide a sufficient statistic, at the same time we have established the applicability of PSR to such complex stateless domains that are mainly addressed using time series analysis or chaos theory. Availability of long range real time data for macro-economics indicators, as well as complex and continous nature of state make economics domain as obvious for experimentation. Moreover Economic Systems are huge systems, it is not possible to find complete state and other machine learning techniques have been employed with only limited success. We found that relatively established fields in RL, Markov decision process (MDP) and Partially Observable Markov Decision Process (POMDP) are not suitable to address problems in economics domain. However we have shown that PSR(Predictive State Representation) along with Artificial Neural Networks (ANNs) can be efficiently employed with very high accuracy in prediction of future interest rate and industrial production average. We have further shown that Support Vector Machine (SVMs) regression can address the problem of prediction of future interest rate with higher accuracy than ANN.
</dd>
<br>
<br>
<dd><a name="swaminathan"></a></dd>
<br>
<dt><dd><b>P. Swaminathan  </b>
</dd> </dt>							
<dd><b>Thesis title:</b>Co-SOFT-Clustering-An Informatic Theoretic Approach To Obtain Overlapping Clusters From Co-occurrence Data
</dd>
<dd>Yahoo Research, Bangalore  </dd>
<br><dd>
<i>ABSTRACT</i>
<br><br>
Co-clustering exploits co-occurrence information, from contingency tables to cluster both rows and columns simultaneously. It has been established that co-clustering produces a better clustering structure as compared to conventional methods of clustering. So far, co-clustering has only been used as a technique for producing hard clusters. In this papaer, we present an algorithm using the information theoretic approach to generate overlapping (soft) clusters. The algorithm maintains probability membership for every instance to each of the possible clusters and iteratively tunes these membership values. The theoretical formulation of the criterion function is presented first, followed by the actual algorithm. We evalute the algorithm over document/word co-occurance information and present experimental results. We discuss the algorithm's applicability for polysemy detection experimental verification. We also discuss directions for future research.
</dd>
<br>
<br>
<dd><a name="shonima"></a></dd>
<dt><b><h2>B.Tech</h2></b>
<br>
<br>
<dt><dd><b>S. Shonima  </b>
</dd> </dt>
<dd><b>Thesis title:</b>Models for Prediction of Angle Closure Glaucoma </dd>
<dd>Strand Life Sciences, Bangalore  </dd>
<br><dd>
<i>ABSTRACT</i>
<br><br>
Angle-closure glaucoma is a condition in which the iris in the eye shifts and blocks the exit passegeway of the aqueous humor,the fluid that fills the eye. This fluid breakup causes a rapid build-up of pressure in the eye. Angle closure glaucoma is an emergency condition that requires immediate medical treatment to preserve vision. Predicting the suspects of the various stages of the angle closure glaucoma at an early stage is important to avoid the progressive loss of vision in both the suspected eye and the other eye.
	The first part of the work deals with building classification models for prediction of angle closure glaucoma in the right eye. A variety of classification algorithms including Support Vector Machines, Random forests, Maximum Margin Classifiers with specified maximum false positive and false negative error rates and Layered Hierarchial Committee of Trees were used to bulid the classification models. In oder to address imbalance, methods like sampling, down sampling and modification of misclassification costs were employed to achieve a high classification accuracy.
	The second part of the work is a risk factor analysis of angle closure glaucoma where a subset of the features that are good predictors of angle closure glaucoma in the right eye are identified.
</dd>
<br>
<br>
<dd><a name="deepthi"></a></dd>
<br>
<br>
<dt><dd><b>Deepthi Cheboli  </b>
</dd> </dt>
<dd><b>Thesis title:</b>Models For Detection Of Feratoconus</dd>
<dd>MS, Univ. of Minnesota, Minneapolis  </dd>
<br><dd>
<i>ABSTRACT</i>
<br><br>
Keratoconus, is a non-inflammatory disorder of the eye in which structural changes within the cornea cause it to thin and change to a more conical shape which leads to substantial distortion of vision. Detection of Keratoconus is important to avoid unpredictable results in refractive surgery. This surgery on a Keratoconic eye might lead to even more thinning of the cornea.
Current methods for the detection of the disease mainly consistes of Neural Networks. We propose a new model, which integrates Manifold learning techniques with Semi-Supervised learning,to maximise the utility of unlabeled data. Through the empirical results,we show that highly accurate models can be bulit using these principles.
</dd>
<br>
<br>
<dd><a name="aniket"></a></dd>
<br>
<br>
<dt><dd><b>Aniket Ray</b>
</dd> </dt>
<dd><b>Thesis title:</b>Models For Prediction Of Retinopathy Of Prematurity</dd>
<dd>Adobe, Mumbai  </dd>
<br><dd>
<i>ABSTRACT</i>
<br><br>
Retinopathy of Prematurity(ROP) is a major condition affecting prematurely born children in the developing world. In this work, we have attempted to use machine learning appraches to learn models for the prediction of Retinopathy of Prematurity.
	We conduct a study on several kinds of classification methods and the empirical results for them have been outlined.
	We propose a hierarchical random committee of trees model to closely model the disease. It has been shown through empirical results that a high level of accuracy is achieved while using this model.
</dd>
<br>
<br>
<br>
<br>
<dd><a name="pranjal"></a></dd>
<br>
<br>
<dt><dd><b>Pranjal Awasthi</b>
</dd> </dt>
<dd><b>Thesis title:</b>Image Segmentation using Multiscale Conditional Random Fields</dd>
<dd>PhD, CMU  </dd>
<br><dd>
<i>ABSTRACT</i>
<br><br>
This work is an attempt to study probablistic graphical models, specifically for image segmentation. The problem of image segmentation is of fundamental importance in computer vision. It is one of the priliminary tasks that are to be performed before any high level processing of the image can be done. Needless to say, good segmentation is crucial for the success of other vision tasks such as object detection and tracking, video sequence analysis etc. In recent years, a lot of research has focused on the use of graphical models for segmenting images. Graphical models combine ideas from graph theory and probability theory in an elegant manner to represent complex systems. Efficient training and interference procedures make them a popular choice for many problem domains.
	Markov random fields (MRFs) are the most popular form of graphical models used for image segmentation. They represent a generative framework by modeling the joint probability of the image pixels and the corresponding labels. There have been a lot of improvements suggested over the basic MRF framework. Some of them include hierarchical MRFs which try to capture information over a range of scales and multiscale random fields (MSRFs) which try to simplify the parameter estimation procedures. In parallel, work has also been done on the use of directed models such as tree structures belief networks(TSBNs) and dynamic trees (DTs). Recently with the introduction of conditional random fields (CRFs), there has been a shift towards the use of conditional models for image segementation. CRFs provide a lot of advantage over generative models.
	Multiscale conditional random field (mCRF) is a recently proposed conditional model for image segmentation. It belongs to the family of the latent state models which are able to capture long range association among pixel labels. After a comprehensive survey of the existing techniquies, we study the mCRF framework in detail and provide an efficient implementation of it. We also provide two possible extensions to the basic model and successfully demonstrate the increase in the performance which they achieve.
</dd>
<br>
<br>
<dd><a name="sowmya"></a></dd>
<br>
<br>
<dt><dd><b>E. Siva Sowmya</b>
</dd> </dt>
<dd><b>Thesis title:</b>Placement And Routing Of 3D-FPGA using Reinforcement Learning and Support Vector Machines</dd>
<dd>PhD, UPenn </dd>
<br><dd>
<i>ABSTRACT</i>
<br><br>
The primary advantage of using 3D-FPGA over 2D-FPGA is that the vertical stacking of active layers reduce the Manhattan distance between the components in 3D-FPGA than when placed on 2D-FPGA. This results in a considerable reduction in total interconnect length. Reduced wire length eventually leads to reduction in delay and hence improved performance and speed. Design of an efficient placement and routing algorithm for 3D-FPGA that fully exploits the above-mentioned advantage is a problem of deep research and commercial interest. In this work, an efficient placement and routing algorithm is proposed for 3D-FPGAs which yields better results in terms of total-interconnect length and channel-width. The proposed algorithm employs two important techniques, namely, Reinforcement Learning(RL) and Support Vector Machines (SVMs), to perform the placement. The proposed algorithm is implemented and tested on standard benchmark circuits and the results obtained are encouraging. This is one of the very few instances where, reinforcement learning is used for solving a problem in the area of VLSI.
</dd>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
</dl>
</body>
</html>
